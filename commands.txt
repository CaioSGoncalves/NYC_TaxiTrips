CREATE CLUSTER (Spark Version: 2.4 + 2 workers + Apache Zeppelin):
gcloud beta dataproc clusters create cluster-3bff --enable-component-gateway --region southamerica-east1 --subnet default --zone southamerica-east1-a --master-machine-type n1-standard-1 --master-boot-disk-size 100 --num-workers 2 --worker-machine-type n1-standard-1 --worker-boot-disk-size 100 --image-version 1.4-debian9 --optional-components ZEPPELIN --project sincere-bongo-264115


CREATE CLUSTER (Spark Version: 2.4 + 1 master node + Apache Zeppelin):
gcloud beta dataproc clusters create cluster-fb19 --enable-component-gateway --region southamerica-east1 --subnet default --zone southamerica-east1-a --single-node --master-machine-type n1-standard-4 --master-boot-disk-size 200 --image-version 1.4-debian9 --optional-components ZEPPELIN --project sincere-bongo-264115


SUBMIT JOB delta_lake_ingestion:
gcloud dataproc jobs submit pyspark \
    --cluster cluster-3bff --region southamerica-east1 \
    --properties spark.jars.packages=io.delta:delta-core_2.11:0.5.0 \
    gs://teste-caio/jobs/delta_lake_ingestion.py


SUBMIT JOB daily_job:
gcloud dataproc jobs submit pyspark \
    --cluster cluster-3bff --region southamerica-east1 \
    --properties spark.jars.packages=io.delta:delta-core_2.11:0.5.0 \
    gs://teste-caio/jobs/daily_job.py


SET Zeppelin spark_submit_packages:
/usr/lib/zeppelin/conf/zeppelin-env.sh
export SPARK_SUBMIT_OPTIONS="--packages io.delta:delta-core_2.11:0.5.0"



